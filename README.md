# Как пользоваться? #

Установка виртуального окужения, активация, установка зависимостей (все делается внутри папки с проектом):

**MacOS/Linux**
```Bash
python -m venv venv
source venv/bin/activate
python -m pip install -r requirements.txt
playwright install
```
**Windows**
```PowerShell
python -m venv venv
.\venv\Scripts\activate
python -m pip install -r requirements.txt
playwright install
```
Есть вероятность что Windows откажет в исполнении файла `.\venv\Scripts\activate` - так происходит, потому что в вашем Windows по-умолчанию стоит запрет на выполнение стрёмных скриптов. Я не настаиваю, но его можно снять, если ввести:
```PowerShell
Set-ExecutionPolicy -ExecutionPolicy RemoteSigned
```
После снятия запрета просто повторите в папке проекта команду `.\venv\Scripts\activate` и все дальнейшие действия.
Запуск проекта делается из папки link_parser. Из нее нужно запустить команду:
```Bash
scrapy crawl link_spider -O result.json
```

# Что это? #
Это паук(/краулер) для скрапинга сайта с категориями пива. На вход он получает ссылки на категории спиртных напитков, а выдает json с информацией по всем напиткам.

# Особенности #
1) `scrapy-playwright` я использую, чтобы справляться с динамическим контентом на странице. Думал использовать `Selenium`, но решил поискать альтернативы.
2) `scrapy-proxy-pool` я использую для проксирования запросов + меняю юзер-агентов.
3) Использую cookie, чтобы получить товары по конкретному региону.

# Как это рабоает? # 
- Скрипт сначала скроллит всю страницу, потом достает все ссылки.
- Дальше он ныряет в каждую ссылку.
- Внутри ссылки нужно нажать на свич, чтобы открыть список всех магазинов - это делает скрипт JS.
- Затем из всей страницы вытаскивается нужная информация и упаковывается в json.

